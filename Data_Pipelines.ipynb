{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipelines\n",
    "\n",
    "### why use pipelines at all? \n",
    "\n",
    "reproducibility! \n",
    "\n",
    "efficiency\n",
    "\n",
    "### how to create pipelines \n",
    "\n",
    "REPL for data exploration, plotting, prototyping \n",
    "\n",
    "IDE for scripting. notebooks can be problematic. also, auto-completion, formatting etc.  \n",
    " \n",
    " - ATOM \n",
    " - pycharm \n",
    " - R studio \n",
    " \n",
    "\n",
    "\n",
    "### directory structure for a project - organizational principles \n",
    "\n",
    "raw_data - immutable. never change these files \n",
    "\n",
    "temp data - \"scratch paper\"  \n",
    "\n",
    "outputs  - what you need to make your figures \n",
    "\n",
    "code - where you keep the code \n",
    "\n",
    "\n",
    "git workflow - save the code and the outputs for sure. if the inputs are large, make sure you have a system for dealing with large data. do not keep temp data (.gitignore) \n",
    "\n",
    "\n",
    "### common computational limitations when dealing with data \n",
    "\n",
    "Memory limitations: \n",
    "\n",
    "RAM - what is it and why is it important \n",
    "\n",
    "can the data fit in memory? \n",
    "\n",
    "HD space - can the data fit on disk? \n",
    "\n",
    "If not, use cloud storage or something. but beware IO speed limitations. \n",
    "\n",
    "Speed limitation - data streaming and processing can fit in memory, but throughput is limited - \n",
    "\n",
    "   parallelization - multicore or cluster computing \n",
    "   \n",
    "Modern options for cluster computing  - institutional clusters, AWS, google cloud, microsoft azure \n",
    "\n",
    "Demo how to spin up an AWS instance (always on) and talk about serverless architecture (amazon lambda) \n",
    "\n",
    "#### evaluating data quality \n",
    "\n",
    "completeness: fraction of missing values \n",
    "              entropy of the dataset \n",
    "\n",
    "representativeness: \n",
    "subsampling and comparison of distributions \n",
    "\n",
    "### Loading data\n",
    "\n",
    "* From files\n",
    "    - Our example\n",
    "* RESTapi\n",
    "    - Example: twitteR\n",
    "* wget/cURL \n",
    "    - Example: DataDryad, Retriever, NASA SEDAC \n",
    "* From database (mention only, no example)\n",
    "    - Example: GDELT/NEON?/BLAST\n",
    "\n",
    "### Preprocessing input data \n",
    "\n",
    "Need to deal with: \n",
    "- null values (missing data) \n",
    "\n",
    "1) collect more data \n",
    "\n",
    "2) imputation \n",
    "\n",
    "3) subsetting \n",
    "\n",
    "\n",
    "- data types = categorical vs. ordinal. str vs int vs. boolean \n",
    "\n",
    "- Sampling bias - how do we know our data is representative of the underlying system? when repeated sampling gives the same distribution (this is the essence of sample size analysis). Also depends on definition of \"same\" and how you measure it (assumptino of gaussian?) \n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  dependencies. \n",
    "\n",
    "when scripting a data pipeline, it's helpful to break down the analysis into separate tasks, and identify the dependencies of each task. ideally, each task only has one dependency -> linear progression of tasks. \n",
    "\n",
    "Add an if statement in the code to check   \n",
    "\n",
    "### dependency graph and Luigi \n",
    "\n",
    " outside the scope of this course, but just FYI - can use framework to handle complex dependencies \n",
    " \n",
    " requires \n",
    " \n",
    " input \n",
    " \n",
    " output \n",
    " \n",
    " \n",
    "### execute processing step, then save intermediate output during processing \n",
    " \n",
    "df=proc_df(df_raw)\n",
    "\n",
    "\n",
    "\n",
    "use /tmp directory if you don't care about intermediate file output. periodically delete these. \n",
    "\n",
    "can use the presence/absence of these files as a monitoring tool - know which part(s) of the pipeline are completed \n",
    "\n",
    "\n",
    "\n",
    "#### monitoring the processes \n",
    "\n",
    "binary outcomes - each sub-task is either complete or not \n",
    " \n",
    " \n",
    "### outputs \n",
    "\n",
    "images \n",
    "processed data (Tables) \n",
    "\n",
    "#### scripting for figures and tables \n",
    "\n",
    "publication ready figures - python and R example scripts for matplotlib and ggplot \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialization \n",
    "\n",
    "\n",
    "## faster, snapshot of memory. new feature in sklearn 20 \n",
    "df.to_feather('tmp/forest-cover')\n",
    "\n",
    "## slower, older. pickle is common in the python ecosystem. \n",
    "df.to_pickle('tmp/forest-cover-pickle.p') \n",
    "\n",
    "# can pickle or feather anything \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
