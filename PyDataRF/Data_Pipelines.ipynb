{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management and Reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility in Jupyter Notebooks  \n",
    "\n",
    "Notebooks are a really great tool. They use REPL for data exploration, plotting, prototyping. \n",
    "\n",
    "REPL  -  Read-Evaluate-Print Loop (see the output of code inline) \n",
    " \n",
    "However, notebooks can be problematic because when you're exploring the data, you can run cells out of order or delete cells with downstream dependencies. This is one reason why professional software developers like to write and test entire scripts instead of line-by-line analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example of cell execution order problems\n",
    "\n",
    "x=3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=2*x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to ensure reproducibility is to go up to the `Kernel` menu and click on `Restart and Run All` to make sure that your notebook is **Linearized** and runs properly when all the cells are executed in order\n",
    "\n",
    "Another option would be to use an IDE to develop reproducible scripts and group scripts and notebooks together in projects. Examples of IDEs: \n",
    "\n",
    " - Atom\n",
    " - Sublime\n",
    " - Vim/nano/emacs/notepad++ \n",
    " - pycharm \n",
    " - RStudio\n",
    " - Jupyter Lab \n",
    " \n",
    " \n",
    " Advantages of IDEs include syntax highlighting, code completion, linting, and integrations with git(hub) and other tools. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "* From files (csv, txt, etc.) \n",
    "    - Our example\n",
    "* REST api (REpresentational State Transfer)\n",
    "    - Example: twitteR, Neon \n",
    "* wget/cURL \n",
    "    - Example: DataDryad, Retriever, NASA SEDAC \n",
    "* From database (mention only, no example)\n",
    "    - Example: GDELT/BLAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neon data manual browsing\n",
    "\n",
    "http://data.neonscience.org/browse-data?showAllDates=true&showAllSites=true&showTheme=org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Structure \n",
    "\n",
    "Break down the data manipulation and analysis into discrete steps \n",
    "\n",
    "Write a function for each step, and present them in order \n",
    "\n",
    "Reuse code when possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if I want to download 100s or 1000s of files? discussion on REST API \n",
    "\n",
    "import requests\n",
    "\n",
    "# must redo API call each time to make sure the download link is authorized and up to date \n",
    "base_url = 'http://data.neonscience.org/api/v0'\n",
    "endpoint = 'data'\n",
    "product_code= 'DP1.00098.001' # relative humidity \n",
    "site_code = 'ABBY' \n",
    "year_month = '2016-07'\n",
    "package = '?package=basic'\n",
    "\n",
    "api_call = str.join('/',[base_url,endpoint,product_code,site_code,year_month,package])\n",
    "print(api_call)\n",
    "r=requests.get(api_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "#with open('json_response.pickle','wb') as handle:\n",
    "#    pickle.dump(r, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell if the response times out \n",
    "with open('json_response.pickle','rb') as handle: \n",
    "    r=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()\n",
    "\n",
    "#url = r.json()['data']['files'][1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all available time periods \n",
    "year_months = [\n",
    "          \"2016-04\",\n",
    "          \"2016-05\",\n",
    "          \"2016-06\",\n",
    "          \"2016-07\",\n",
    "          \"2016-08\",\n",
    "          \"2016-09\",\n",
    "          \"2016-10\",\n",
    "          \"2016-11\",\n",
    "          \"2016-12\",\n",
    "          \"2017-01\",\n",
    "          \"2017-02\",\n",
    "          \"2017-03\",\n",
    "          \"2017-04\",\n",
    "          \"2017-05\",\n",
    "          \"2017-06\",\n",
    "          \"2017-12\",\n",
    "          \"2018-01\",\n",
    "          \"2018-02\",\n",
    "          \"2018-03\",\n",
    "          \"2018-04\",\n",
    "          \"2018-05\",\n",
    "          \"2018-06\",\n",
    "          \"2018-07\",\n",
    "          \"2018-08\",\n",
    "          \"2018-09\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wget example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url.split('?')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('wget '+url.split('?')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "file_name='ABBY_rel_humid_2016-07_RAW.csv' # distinguish raw data \n",
    "\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing input data \n",
    "\n",
    "Need to deal with: \n",
    "- null values (missing data) \n",
    "\n",
    "1) collect more data \n",
    "\n",
    "2) imputation \n",
    "\n",
    "3) subsetting \n",
    "\n",
    "\n",
    "- data types = categorical vs. ordinal. str vs int vs. boolean \n",
    "\n",
    "- Sampling bias - how do we know our data is representative of the underlying system? when repeated sampling gives the same distribution (this is the essence of sample size analysis). Also depends on definition of \"same\" and how you measure it (assumptino of gaussian?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ABBY_rel_humid_2016-07_RAW.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "display_all(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating data quality \n",
    "\n",
    "completeness: \n",
    "- fraction of missing values \n",
    "\n",
    "consistency:\n",
    "- unique values for each category \n",
    "- numbers represented with the same data type \n",
    "- draw 2 sets of random samples from data and compare the distributions \n",
    "\n",
    "representativeness/accuracy: \n",
    "- compare data from different time periods or different sources \n",
    "- calibration with 2nd data source (ground truthing) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()['RHMean']/len(df) # wow, 20% null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "idxs_null=df[df['RHMaximum'].isnull()].index\n",
    "plt.hist(idxs_null) # what does this tell us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idxs_null]['startDateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[:35572]\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('rel_humid_ABBY_2017-05.csv') # save intermediate output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RHMean'].median()  #metric we want to aggregate for all time periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians=[] #initialize an empty list\n",
    "\n",
    "medians.append({year_month: df['RHMean'].median()}) # add the median for this file to the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.pairplot(df[['RHMean','tempRHMean','dewTempMean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sample(df, n):\n",
    "    idxs = sorted(np.random.permutation(len(df))[:n])\n",
    "    return df.iloc[idxs].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(get_sample(df[['RHMean','tempRHMean','dewTempMean']],1000))\n",
    "g = g.map_upper(plt.scatter)\n",
    "g = g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g = g.map_diag(sns.kdeplot, lw=3, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf()\n",
    "plt.savefig('RH_pairplot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task separation and  dependency management  \n",
    "\n",
    "when scripting a data pipeline, it's helpful to break down the analysis into separate tasks, and identify the dependencies of each task. \n",
    "\n",
    "\n",
    "Linear progression of tasks: each task only has one dependency. \n",
    "\n",
    "If you have multiple dependencies, then managing everything manually gets messy as the # of tasks increases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "### execute processing step, then save intermediate output during processing \n",
    "\n",
    "use /tmp directory if you don't care about intermediate file output. periodically delete these. \n",
    "\n",
    "can use the presence/absence of these files as a monitoring tool - know which part(s) of the pipeline are completed \n",
    "\n",
    "e.g. if the intermediate .csv file exists then, we know that processing happened. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv() method to save as csv  \n",
    "\n",
    "but what if I want to save my model object or any other object in python? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## faster, snapshot of memory. new feature in sklearn 20 \n",
    "df.to_feather('tmp/forest-cover')\n",
    "\n",
    "## slower, older. pickle is common in the python ecosystem. \n",
    "df.to_pickle('tmp/forest-cover-pickle.p') \n",
    "\n",
    "# can pickle or feather anything \n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('RH_graph_object.pickle','wb') as handle: \n",
    "    pickle.dump(g, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's turn this data exploration into reproducible code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### should I run script on my machine or on a cluster?  \n",
    "\n",
    "#### Memory limitations: \n",
    "\n",
    "RAM - what is it and why is it important \n",
    "\n",
    "can the data fit in memory? \n",
    "\n",
    "HD space - can the data fit on disk? \n",
    "\n",
    "If not, use cloud storage or something. but beware IO speed limitations. \n",
    "\n",
    "#### Speed limitation - data streaming and processing can fit in memory, but throughput is limited - \n",
    "\n",
    "parallelization - multicore or cluster computing \n",
    "   \n",
    "Modern options for cluster computing  - institutional clusters, AWS, google cloud, microsoft azure \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory structure for a project - organizational suggestions\n",
    "\n",
    "`/data/raw/` - immutable. never change these files \n",
    "\n",
    "`/code` or `/scripts` \n",
    "\n",
    "`/tmp` - temporary folder i.e. \"scratch paper\" \n",
    "\n",
    "`/data/clean/` - post-processed data \n",
    "\n",
    "`/figures`  \n",
    "\n",
    "git workflow - save the code and the outputs for sure. if the inputs are large, make sure you have a system for dealing with large data. do not keep temp data (.gitignore) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create folders in terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
