{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management and Reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory structure for a project - organizational suggestions\n",
    "\n",
    "`/data/raw/` - immutable. never change these files \n",
    "\n",
    "`/code` or `/scripts` \n",
    "\n",
    "`/tmp` - temporary folder i.e. \"scratch paper\" \n",
    "\n",
    "`/data/clean/` - post-processed data \n",
    "\n",
    "`/figures`  \n",
    "\n",
    "git workflow - save the code and the outputs for sure. if the inputs are large, make sure you have a system for dealing with large data. do not keep temp data (.gitignore) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create folders in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility in Jupyter Notebooks  \n",
    "\n",
    "Notebooks are a really great tool. They use REPL for data exploration, plotting, prototyping. \n",
    "\n",
    "REPL  -  Read-Evaluate-Print Loop (see the output of code inline) \n",
    " \n",
    "However, notebooks can be problematic because when you're exploring the data, you can run cells out of order or delete cells with downstream dependencies. This is one reason why professional software developers like to write and test entire scripts instead of line-by-line analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example of cell execution order problems\n",
    "\n",
    "x=3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=2*x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to ensure reproducibility is to go up to the `Kernel` menu and click on `Restart and Run All` to make sure that your notebook is **Linearized** and runs properly when all the cells are executed in order\n",
    "\n",
    "Another option would be to use an IDE to develop reproducible scripts and group scripts and notebooks together in projects. Examples of IDEs: \n",
    "\n",
    " - Atom\n",
    " - Sublime\n",
    " - Vim/nano/emacs/notepad++ \n",
    " - pycharm \n",
    " - RStudio\n",
    " - Jupyter Lab \n",
    " \n",
    " \n",
    " Advantages of IDEs include syntax highlighting, code completion, linting, and integrations with git(hub) and other tools. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "* From files (csv, txt, etc.) \n",
    "    - Our example\n",
    "* REST api (REpresentational State Transfer)\n",
    "    - Example: twitteR, Neon \n",
    "* wget/cURL \n",
    "    - Example: DataDryad, Retriever, NASA SEDAC \n",
    "* From database (mention only, no example)\n",
    "    - Example: GDELT/BLAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neon data manual browsing\n",
    "\n",
    "http://data.neonscience.org/browse-data?showAllDates=true&showAllSites=true&showTheme=org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://data.neonscience.org/api/v0/data/DP1.00098.001/ABBY/2016-07/?package=basic\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# must redo API call each time to make sure the download link is authorized and up to date \n",
    "base_url = 'http://data.neonscience.org/api/v0'\n",
    "endpoint = 'data'\n",
    "product_code= 'DP1.00098.001' # relative humidity \n",
    "site_code = 'ABBY' \n",
    "year_month = '2016-07'\n",
    "package = '?package=basic'\n",
    "\n",
    "api_call = str.join('/',[base_url,endpoint,product_code,site_code,year_month,package])\n",
    "print(api_call)\n",
    "r=requests.get(api_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=r.json()['data']['files'][1]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('json_response.pickle','wb') as handle:\n",
    "    pickle.dump(r, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_response.pickle','rb') as handle: \n",
    "    r=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wget example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://neon-prod-pub-1.s3.data.neonscience.org/NEON.DOM.SITE.DP1.00098.001/PROV/ABBY/20160701T000000--20160801T000000/basic/NEON.D16.ABBY.DP1.00098.001.000.040.001.RH_1min.2016-07.basic.20171026T093814Z.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url.split('?')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('wget '+url.split('?')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "file_name='ABBY_rel_humid_2016-07_RAW.csv' # distinguish raw data \n",
    "\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing input data \n",
    "\n",
    "Need to deal with: \n",
    "- null values (missing data) \n",
    "\n",
    "1) collect more data \n",
    "\n",
    "2) imputation \n",
    "\n",
    "3) subsetting \n",
    "\n",
    "\n",
    "- data types = categorical vs. ordinal. str vs int vs. boolean \n",
    "\n",
    "- Sampling bias - how do we know our data is representative of the underlying system? when repeated sampling gives the same distribution (this is the essence of sample size analysis). Also depends on definition of \"same\" and how you measure it (assumptino of gaussian?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ABBY_rel_humid_2016-07_RAW.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "display_all(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating data quality \n",
    "\n",
    "completeness: \n",
    "- fraction of missing values \n",
    "- entropy of the dataset \n",
    "\n",
    "representativeness: \n",
    "- subsampling and comparison of distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()['RHMean']/len(df) # wow, 20% null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "idxs_null=df[df['RHMaximum'].isnull()].index\n",
    "plt.hist(idxs_null) # what does this tell us? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idxs_null]['startDateTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[:35572]\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('rel_humid_ABBY_2017-05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['RHMean','tempRHMean','dewTempMean',\n",
    "                     'RHStdErMean','tempRHStdErMean','dewTempStdErMean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.pairplot(df[['RHMean','tempRHMean','dewTempMean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(get_sample(df[['RHMean','tempRHMean','dewTempMean']],1000))\n",
    "g = g.map_upper(plt.scatter)\n",
    "g = g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "g = g.map_diag(sns.kdeplot, lw=3, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf()\n",
    "plt.savefig('RH_pairplot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Now let's "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task separation and  dependency management  \n",
    "\n",
    "when scripting a data pipeline, it's helpful to break down the analysis into separate tasks, and identify the dependencies of each task. \n",
    "\n",
    "\n",
    "Linear progression of tasks: each task only has one dependency. \n",
    "\n",
    "If you have multiple dependencies, then managing everything manually gets messy as the # of tasks increases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "??proc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "### execute processing step, then save intermediate output during processing \n",
    " \n",
    "df=proc_df(df_raw)\n",
    "\n",
    "use /tmp directory if you don't care about intermediate file output. periodically delete these. \n",
    "\n",
    "can use the presence/absence of these files as a monitoring tool - know which part(s) of the pipeline are completed \n",
    "\n",
    "\n",
    "\n",
    "#### monitoring the processes \n",
    "\n",
    "binary outcomes - each sub-task is either complete or not \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## faster, snapshot of memory. new feature in sklearn 20 \n",
    "df.to_feather('tmp/forest-cover')\n",
    "\n",
    "## slower, older. pickle is common in the python ecosystem. \n",
    "df.to_pickle('tmp/forest-cover-pickle.p') \n",
    "\n",
    "# can pickle or feather anything \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## should I run script on my machine or on a cluster?  \n",
    "\n",
    "#### Memory limitations: \n",
    "\n",
    "RAM - what is it and why is it important \n",
    "\n",
    "can the data fit in memory? \n",
    "\n",
    "HD space - can the data fit on disk? \n",
    "\n",
    "If not, use cloud storage or something. but beware IO speed limitations. \n",
    "\n",
    "#### Speed limitation - data streaming and processing can fit in memory, but throughput is limited - \n",
    "\n",
    "parallelization - multicore or cluster computing \n",
    "   \n",
    "Modern options for cluster computing  - institutional clusters, AWS, google cloud, microsoft azure \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
